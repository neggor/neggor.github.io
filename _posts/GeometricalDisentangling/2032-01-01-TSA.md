---
layout: post
title: Toroidal subgroup analysis
tag: [Physics, ML]
---
<link href="/css/syntax.css" rel="stylesheet" >
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


TODOS:
1- Motivation with Lie theory (I think the most important think here is to motivate this from the perspective of group theory...)
(Maybe also comment on the invariant metric...)
2- Derivation of likelihood
3- Results
4- Comments
(This actually should not be too difficult!)
(the idea is basically to wrap up the progress in group theory and show off a bit what can be done here... not much, do not over do it)

## References
The following notes and code are based on https://arxiv.org/pdf/1402.4437 , https://arxiv.org/pdf/2012.12071 and some bits from https://arxiv.org/abs/2104.13478 .
I got some background on group theory from http://abstract.ups.edu/aata/aata-toc.html. For introducing myself to representation theory, Lie groups and Lie algebras I
used https://sites.ualberta.ca/~vbouchar/MAPH464/notes.html .

## Introduction

After watching the amazing 3h documentary in Geometric Deep learning (https://www.youtube.com/watch?v=bIZB1hIJ4u8) I got very trapped by the idea of "transformations that do not alter the meaning of an object" and how that relates to the way we understand the world. 

In a very basic sense, this can be approximated through group theory and the idea of the orbit, the set of "views" that is constructed by applying all the possible elements of a group $$G$$ to an element $$x \in X$$ i.e $$\{y = g * x \forall g \in G\}$$. The basic idea being that the face of Jimmy is the same
either he is angry or not, he inclines his head and so on (would be instances of $$g \in G$$). All these prossible transformations form a group that do not alter the fact that this is still Jimmy's face. As put in the work by Welling and Cohen referenced above, the idea is
> to build a representation that is invariant to the transformation of a factor of variation (e.g. object position) that is considered a nuisance for a particular task (e.g. object classification), one can simply ignore the units in the representation that encode the nuisance factor.

These ideas go very deep in the geometric deep learning literature. Indeed, the inner workings of succesful deep learning architectures in very high dimensional domains are based on some notion of invariant representations (I will be making study notes on that...). This is even more interesting
when one reflects on the underpinnings of the capabilities of humans to learn. Inarguably, evolution 
has provided us with some structure which allows us to recognize the same concepts under different scenarios. Once you learn about groups, it is difficult to unseen this stuff...

While looking into the references of the above mentioned papers, I got to a work by Warren Sturgis McCulloch (1898-1969) on ["How we know universals. The perception of auditory and visual forms"](https://link.springer.com/article/10.1007/BF02478291). In that paper ...

Make a summary of that paper here, I think it can be quite interesting... also because I can use it to introduce concepts on lie alegebras


Anyway, let's get some definitions going and to the ML stuff.
## Mathematical background
Here mostly definitions and stuff that are needed and that I'd like to have a more cleare picture of. Without much rigurosity...

The following description do not aim for rigurosity but intuition!
### Group and Group representation
### Lie Group and Lie Algebra
### Von Misses Distribution

## Toroidal subgroup analysis (TSA); a toy example
The objective is to obtain the probability $$P(\mathbf{y} | \mathbf{x})$$ for $$\mathbf{y, x}$$ being black and white images in $$\mathbb{R}^n$$ (being n even, for simplicity). The interesting part is that we want this probability to be invariant to the effects of rotation, for any angle. Hence, this boils down to relate two images by a relationship such that $$\mathbf{y}$$ is on the _orbit_ of $$\mathbf{x}$$ for some group $$G$$, in this case SO(n).

In practical terms we will rotate an image around its center using a 2D rotation matrix $$R(\theta)$$ to construct $$\mathbf{x}$$ from $$\mathbf{y}$$. Then we will use 

$$ y = \mathbf{WR(s)W^T}x $$

(need to specify if this s is a vector or an scalar, and why/where does that come from)

where $$\mathbf{W}$$ is an othogonal matrix and $$\mathbf{R(s)}$$ is a block diagonal matrix of 2D rotations. Note that while the dataset is constructed using the pixel indices, here we use the image in vector representation, which is an n-dimensional vector. Hence, this rotation is happening in a n-dimensional space.
(I need to clarify this...)

## The model
The idea is to integrate over the orbit of $$\mathbf{x}$$ to obtain the probability $$P(\mathbf{y} | \mathbf{x})$$. This is done by integrating over the group $$G$$, in this case SO(n).

To do this, first we need to define a probability distribution for the angle of rotation. This is an interesting problem, since 1 and $$2\pi$$ are very close but are quite separated in absolute terms. We cannot model this behaviour with a Gaussian distribution (or any distribution that is based on euclidean distances). The von Mises probability density for an angle $$x$$ is given by:

(delve more into this distribution and its motivation, how it arises et... is actually pretty interesting) INDEED, tottally worth it to stop on the issue of this distribution

$$M(x | \mu, k) = \frac{\exp(k \cos (x - \mu))}{2\pi I_0(k)}$$

[1^]: This comes from ...

(Also, define $$\mathbf{u, v}$$ and so on, following the paper)

This can be put in the form of the exponential family as:

$$p{\theta} = \frac{1}{2\pi I_0(\left \lVert \eta \right \lVert)} \exp(\eta T(\theta))$$

For $$T(\theta) = (\cos(\theta), sin(\theta))^T$$, 
and $$\eta$$ defines the average:
$$\eta = k[\cos(\mu), sin(\mu)]^T$$

(Now, generalize this to arbitrary dimensions of theta!)

And it turns out (following ...) that (Also show derivation here) for a normal likelihood we can find the *posterior of this angle*:

$$p(\theta | \mathbf{x}, \mathbf{y}) \propto 
p(\mathbf{y}| \mathbf{x}, \theta)p(\theta) =
$$

(Using bayes theorem, confirm this)

$$
\exp(-\frac{1}{2\sigma^2}\left \lVert y - \mathbf{WR(\theta)W^Tx} \right \lVert^2)p(\theta)
$$

(continue with the derivation following Max Welling stuff, also make a more detailed analysis of the geometrical interpretation, maybe using plots..?)

Now the *key* here is that we can find a close form solution for the *updated average* $$\hat{\mu}$$ and hence for $$\hat{\eta}$$. This is due to how the likelihood is defined, deriving this analytically is known as conjugate updating:

(Show the derivation with all the steps, make sure they are correct)

So, turns out that the posterior mean is a function of the prior and $$mathbf{u, v}$$. That is great, because now we've got the distribution of the angle as a function of $$mathbf{W, x, y}$$ . 

(In the LSC paper, then in the derivation of the integral it arises naturally the definition of the posterior, this is not shown in the Welling paper, which makes it difficult to follow. The upsetting part is that they require numerical intergration for that part...)

(In any case, the relevant point here is that the definition of the likelihood integrating over the elements of the group makes sense because it implies a definition of the average posterior (which relates to the angle), and that is itself just a function of x, y and W. This is kind of magical tbh... )

### coupled vs decoupled?
(Because I am going to follow the slightly more generalized version, I could probably skip all that??
 But make sure to be consistent!)

 Nah probably fuck the coupled one, too hard, I am happy enough with the decoupled one
## Why does this work?